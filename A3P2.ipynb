{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3P2_jp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "p-sdLtMH5j_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Disclaimer\n",
        "\n",
        "We used elements of the code from:\n",
        "https://github.com/yoonholee/pytorch-vae\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vn3eTJ5154sg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import"
      ]
    },
    {
      "metadata": {
        "id": "LTW8u2_da4MH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXi5lbk7BClQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data loader\n",
        "\n",
        "Taken from  https://raw.githubusercontent.com/yoonholee/pytorch-vae/master/data_loader/fixed_mnist.py"
      ]
    },
    {
      "metadata": {
        "id": "InaYpgmRA90R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class fixedMNIST(data.Dataset):\n",
        "    \"\"\" Binarized MNIST dataset, proposed in\n",
        "    http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf\n",
        "\n",
        "    Based on https://raw.githubusercontent.com/yoonholee/pytorch-vae/master/data_loader/fixed_mnist.py\n",
        "\n",
        "    \"\"\"\n",
        "    train_file = 'binarized_mnist_train.amat'\n",
        "    val_file = 'binarized_mnist_valid.amat'\n",
        "    test_file = 'binarized_mnist_test.amat'\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, download=False):\n",
        "        # we ignore transform.\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
        "\n",
        "        self.data = self._get_data(train=train)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index]\n",
        "        img = Image.fromarray(img)\n",
        "        img = transforms.ToTensor()(img).type(torch.FloatTensor)\n",
        "        return img, torch.tensor(-1)  # Meaningless tensor instead of target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _get_data(self, train=True):\n",
        "        with h5py.File(os.path.join(self.root, 'data.h5'), 'r') as hf:\n",
        "            data = hf.get('train' if train else 'test')\n",
        "            data = np.array(data)\n",
        "        return data\n",
        "\n",
        "    def get_mean_img(self):\n",
        "        return self.data.mean(0).flatten()\n",
        "\n",
        "    def download(self):\n",
        "        if self._check_exists():\n",
        "            return\n",
        "        if not os.path.exists(self.root):\n",
        "            os.makedirs(self.root)\n",
        "\n",
        "        print('Downloading MNIST with fixed binarization...')\n",
        "        for dataset in ['train', 'valid', 'test']:\n",
        "            filename = 'binarized_mnist_{}.amat'.format(dataset)\n",
        "            url = 'http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_{}.amat'.format(dataset)\n",
        "            print('Downloading from {}...'.format(url))\n",
        "            local_filename = os.path.join(self.root, filename)\n",
        "            urllib.request.urlretrieve(url, local_filename)\n",
        "            print('Saved to {}'.format(local_filename))\n",
        "\n",
        "        def filename_to_np(filename):\n",
        "            with open(filename) as f:\n",
        "                lines = f.readlines()\n",
        "            return np.array([[int(i)for i in line.split()] for line in lines]).astype('int8')\n",
        "\n",
        "        train_data = np.concatenate([filename_to_np(os.path.join(self.root, self.train_file)),\n",
        "                                     filename_to_np(os.path.join(self.root, self.val_file))])\n",
        "        test_data = filename_to_np(os.path.join(self.root, self.val_file))\n",
        "        with h5py.File(os.path.join(self.root, 'data.h5'), 'w') as hf:\n",
        "            hf.create_dataset('train', data=train_data.reshape(-1, 28, 28))\n",
        "            hf.create_dataset('test', data=test_data.reshape(-1, 28, 28))\n",
        "        print('Done!')\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, 'data.h5'))\n",
        "\n",
        "\n",
        "def data_loaders():\n",
        "\n",
        "    loader_fn, root = fixedMNIST, './dataset/fixedmnist'\n",
        "\n",
        "    dataset_dir = r'/content/'\n",
        "    root = dataset_dir\n",
        "    kwargs = {}\n",
        "\n",
        "    batch_size = 32\n",
        "    test_batch_size = 32\n",
        "\n",
        "    # Train set\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        loader_fn(root, train=True,\n",
        "                  download=True,\n",
        "                  transform=transforms.ToTensor()),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs)\n",
        "\n",
        "    # Test set\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        loader_fn(root,\n",
        "                  train=False,\n",
        "                  download=True,\n",
        "                  transform=transforms.ToTensor()),\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False,\n",
        "        **kwargs)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def show_input(loader):\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(7,7), squeeze=False)\n",
        "\n",
        "    ax[0, 0].imshow(loader.dataset[11][0].reshape((28,28)),\n",
        "               cmap='gray', interpolation='nearest')\n",
        "    ax[0, 1].imshow(loader.dataset[1121][0].reshape((28,28)),\n",
        "               cmap='gray', interpolation='nearest')\n",
        "    ax[1, 0].imshow(loader.dataset[111][0].reshape((28,28)),\n",
        "               cmap='gray', interpolation='nearest')\n",
        "    ax[1, 1].imshow(loader.dataset[121][0].reshape((28,28)),\n",
        "               cmap='gray', interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NA_PY_26bEN7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# VAE Class"
      ]
    },
    {
      "metadata": {
        "id": "mKuwlXALZYui",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), 16 * 8 * 8).contiguous()\n",
        "\n",
        "\n",
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), 256, 1, 1).contiguous()\n",
        "\n",
        "\n",
        "class ConvVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, device='cuda', z_dim=100, elbo=True):\n",
        "\n",
        "        super().__init__()\n",
        "        self.train_step = 0\n",
        "        self.best_loss = np.inf\n",
        "        self.elbo=elbo\n",
        "        self.device = device\n",
        "        # Prior on P(Z) => N(0, 1)\n",
        "        self.prior = Normal(torch.zeros([z_dim], device = self.device ), \\\n",
        "                            torch.ones([z_dim], device = self.device ))\n",
        "        \n",
        "        self.proc_data = lambda x: x.to( self.device )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3)),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(kernel_size=(2, 2), stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3)),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(kernel_size=(2, 2), stride=2),\n",
        "            nn.Conv2d(64, 256, kernel_size=(5, 5)),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        # Distribution parameters\n",
        "        self.enc_mu = nn.Linear(in_features=256, out_features=100)\n",
        "        self.enc_log_var = nn.Linear(in_features=256, out_features=100)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=100, out_features=256),\n",
        "            UnFlatten(),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(256, 64, kernel_size=(5, 5), padding=(4, 4)),\n",
        "            nn.ELU(),\n",
        "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            nn.Conv2d(64, 32, kernel_size=(3, 3), padding=(2, 2)),\n",
        "            nn.ELU(),\n",
        "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            nn.Conv2d(32, 16, kernel_size=(3, 3), padding=(2, 2)),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(16, 1, kernel_size=(3, 3), padding=(2, 2)))\n",
        "        \n",
        "    def encode(self, x):\n",
        "        x = self.proc_data(x)\n",
        "        # Encoder\n",
        "        h = self.encoder(x)\n",
        "        # Reshape before linear.\n",
        "        h = h.view(32, 256)\n",
        "        # Distribution parameters.\n",
        "        mu = self.enc_mu(h)\n",
        "        log_var = self.enc_log_var(h)\n",
        "\n",
        "        return mu, log_var\n",
        "\n",
        "    def decode(self, z):\n",
        "        x_hat = self.decoder(z)\n",
        "        return torch.sigmoid(x_hat)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        mu, log_var = self.encode(x)\n",
        "        std = 1e-10 + torch.sqrt(torch.exp(log_var))\n",
        "        # Simulate K random\n",
        "        loss = 0\n",
        "        e = torch.randn((32, 100)).to( self.device )\n",
        "        z = mu + std * e\n",
        "        # DECODE\n",
        "        x_hat = self.decode(z)\n",
        "        loss += self.loss(x_hat, x, mu, std)\n",
        "\n",
        "        return loss, x_hat, mu, std\n",
        "    \n",
        "    def loss(self, x_hat, x, mu, std):\n",
        "        \"\"\"\n",
        "        ELBO \n",
        "        \"\"\"\n",
        "        x = self.proc_data(x)\n",
        "        recon_loss = nn.functional.binary_cross_entropy(x_hat,\n",
        "                                                        x,\n",
        "                                                        reduction='sum')\n",
        "        kl_loss = 0.5 * torch.sum(-1. - torch.log(std**2) + mu ** 2 + std**2)\n",
        "        loss = recon_loss + kl_loss\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h1z1B_woblLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ]
    },
    {
      "metadata": {
        "id": "wl2KeGSnbpG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GPU?"
      ]
    },
    {
      "metadata": {
        "id": "PZ6gyO8dyvST",
        "colab_type": "code",
        "outputId": "1df6f973-ed29-4ed1-f209-18ec46c519dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RpJJ86T0bxle",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train"
      ]
    },
    {
      "metadata": {
        "id": "M-HKqMVUZY5A",
        "colab_type": "code",
        "outputId": "1c636c55-a347-47db-c78c-1fc57c795ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Use the GPU if you have one\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU\")\n",
        "    device = torch.device(\"cuda\") \n",
        "else:\n",
        "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
        "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, x_hat, mu, std = model(data)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.train_step += 1\n",
        "        if model.train_step % 100 == 0:\n",
        "            print('Train Epoch: {} ({:.0f}%)\\tELBO: {:.6f}'.format(\n",
        "                epoch, \n",
        "                100. * batch_idx / len(train_loader), \n",
        "                loss.item() / 32))\n",
        "\n",
        "    # Normalize per instance\n",
        "    print('====> Epoch: {} - Train set - Per instance ELBO: {:.4f}'.format(\n",
        "        epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, _) in enumerate(test_loader):\n",
        "            if data.size(0) == 32: \n",
        "                data = data.to(device)\n",
        "                loss, x_hat, mu, std = model(data)\n",
        "                test_loss += loss.item()\n",
        "                if batch_idx == 0:\n",
        "                    n = min(data.size(0), 32)\n",
        "                    comparison = torch.cat(\n",
        "                        [data[:n], x_hat.view(32, 1, 28, 28)[:n]]\n",
        "                    )\n",
        "                    save_image(comparison.cpu(),\n",
        "                               r'/content/' + str(epoch) + '.png',\n",
        "                               nrow=n)\n",
        "    # Normalize per instance.\n",
        "    test_loss /= len(test_loader.dataset)                \n",
        "    print('====> Epoch: {} - Test set - Per Instance ELBO: {:.4f}'.format(\n",
        "        epoch, test_loss))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aWuhNXbdArA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run training"
      ]
    },
    {
      "metadata": {
        "id": "jcHkBtxBAnxE",
        "colab_type": "code",
        "outputId": "7176ef06-966b-4cfa-e7d1-b47e1d3b9ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7242
        }
      },
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = data_loaders()\n",
        "n_epochs = 20\n",
        "\n",
        "model = ConvVAE(device=device,\n",
        "                z_dim=100,\n",
        "                elbo=True)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3*1e-4, eps=1e-4)\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        \n",
        "torch.save(model.state_dict(), r'/content/model_simple')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading MNIST with fixed binarization...\n",
            "Downloading from http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_train.amat...\n",
            "Saved to /content/binarized_mnist_train.amat\n",
            "Downloading from http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_valid.amat...\n",
            "Saved to /content/binarized_mnist_valid.amat\n",
            "Downloading from http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_test.amat...\n",
            "Saved to /content/binarized_mnist_test.amat\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 (5%)\tELBO: 212.066666\n",
            "Train Epoch: 1 (11%)\tELBO: 171.716995\n",
            "Train Epoch: 1 (16%)\tELBO: 186.088882\n",
            "Train Epoch: 1 (21%)\tELBO: 162.172287\n",
            "Train Epoch: 1 (27%)\tELBO: 160.602661\n",
            "Train Epoch: 1 (32%)\tELBO: 143.329346\n",
            "Train Epoch: 1 (37%)\tELBO: 149.581604\n",
            "Train Epoch: 1 (43%)\tELBO: 134.818954\n",
            "Train Epoch: 1 (48%)\tELBO: 129.019791\n",
            "Train Epoch: 1 (53%)\tELBO: 139.632126\n",
            "Train Epoch: 1 (59%)\tELBO: 124.642929\n",
            "Train Epoch: 1 (64%)\tELBO: 125.998009\n",
            "Train Epoch: 1 (69%)\tELBO: 133.418945\n",
            "Train Epoch: 1 (75%)\tELBO: 121.071045\n",
            "Train Epoch: 1 (80%)\tELBO: 127.615189\n",
            "Train Epoch: 1 (85%)\tELBO: 130.657379\n",
            "Train Epoch: 1 (91%)\tELBO: 124.666245\n",
            "Train Epoch: 1 (96%)\tELBO: 116.282753\n",
            "====> Epoch: 1 - Train set - Per instance ELBO: 153.5802\n",
            "====> Epoch: 1 - Test set - Per Instance ELBO: 118.2221\n",
            "Train Epoch: 2 (1%)\tELBO: 112.939766\n",
            "Train Epoch: 2 (7%)\tELBO: 117.157059\n",
            "Train Epoch: 2 (12%)\tELBO: 117.544724\n",
            "Train Epoch: 2 (17%)\tELBO: 106.003166\n",
            "Train Epoch: 2 (23%)\tELBO: 115.502991\n",
            "Train Epoch: 2 (28%)\tELBO: 113.170044\n",
            "Train Epoch: 2 (33%)\tELBO: 104.893372\n",
            "Train Epoch: 2 (39%)\tELBO: 121.572205\n",
            "Train Epoch: 2 (44%)\tELBO: 97.016205\n",
            "Train Epoch: 2 (49%)\tELBO: 111.619827\n",
            "Train Epoch: 2 (55%)\tELBO: 113.425644\n",
            "Train Epoch: 2 (60%)\tELBO: 113.996704\n",
            "Train Epoch: 2 (65%)\tELBO: 107.831520\n",
            "Train Epoch: 2 (71%)\tELBO: 109.085091\n",
            "Train Epoch: 2 (76%)\tELBO: 118.117447\n",
            "Train Epoch: 2 (81%)\tELBO: 111.614044\n",
            "Train Epoch: 2 (87%)\tELBO: 107.724731\n",
            "Train Epoch: 2 (92%)\tELBO: 105.503342\n",
            "Train Epoch: 2 (97%)\tELBO: 112.105591\n",
            "====> Epoch: 2 - Train set - Per instance ELBO: 111.5788\n",
            "====> Epoch: 2 - Test set - Per Instance ELBO: 106.6029\n",
            "Train Epoch: 3 (3%)\tELBO: 113.492935\n",
            "Train Epoch: 3 (8%)\tELBO: 108.856224\n",
            "Train Epoch: 3 (13%)\tELBO: 99.962044\n",
            "Train Epoch: 3 (19%)\tELBO: 102.509521\n",
            "Train Epoch: 3 (24%)\tELBO: 110.031143\n",
            "Train Epoch: 3 (29%)\tELBO: 110.194626\n",
            "Train Epoch: 3 (35%)\tELBO: 112.747299\n",
            "Train Epoch: 3 (40%)\tELBO: 107.099594\n",
            "Train Epoch: 3 (45%)\tELBO: 102.278122\n",
            "Train Epoch: 3 (51%)\tELBO: 111.971985\n",
            "Train Epoch: 3 (56%)\tELBO: 101.906906\n",
            "Train Epoch: 3 (61%)\tELBO: 97.713379\n",
            "Train Epoch: 3 (67%)\tELBO: 93.901619\n",
            "Train Epoch: 3 (72%)\tELBO: 100.386200\n",
            "Train Epoch: 3 (77%)\tELBO: 108.656876\n",
            "Train Epoch: 3 (83%)\tELBO: 105.384621\n",
            "Train Epoch: 3 (88%)\tELBO: 112.290176\n",
            "Train Epoch: 3 (93%)\tELBO: 98.901939\n",
            "Train Epoch: 3 (99%)\tELBO: 100.347778\n",
            "====> Epoch: 3 - Train set - Per instance ELBO: 104.6969\n",
            "====> Epoch: 3 - Test set - Per Instance ELBO: 102.4318\n",
            "Train Epoch: 4 (4%)\tELBO: 97.741554\n",
            "Train Epoch: 4 (9%)\tELBO: 92.894020\n",
            "Train Epoch: 4 (15%)\tELBO: 101.771141\n",
            "Train Epoch: 4 (20%)\tELBO: 98.042000\n",
            "Train Epoch: 4 (25%)\tELBO: 105.169937\n",
            "Train Epoch: 4 (31%)\tELBO: 100.912979\n",
            "Train Epoch: 4 (36%)\tELBO: 103.451508\n",
            "Train Epoch: 4 (41%)\tELBO: 102.629723\n",
            "Train Epoch: 4 (47%)\tELBO: 105.931793\n",
            "Train Epoch: 4 (52%)\tELBO: 92.881538\n",
            "Train Epoch: 4 (57%)\tELBO: 93.117538\n",
            "Train Epoch: 4 (63%)\tELBO: 98.207466\n",
            "Train Epoch: 4 (68%)\tELBO: 110.851326\n",
            "Train Epoch: 4 (73%)\tELBO: 97.498070\n",
            "Train Epoch: 4 (79%)\tELBO: 106.240700\n",
            "Train Epoch: 4 (84%)\tELBO: 113.351273\n",
            "Train Epoch: 4 (89%)\tELBO: 108.212570\n",
            "Train Epoch: 4 (95%)\tELBO: 100.359314\n",
            "Train Epoch: 4 (100%)\tELBO: 99.394821\n",
            "====> Epoch: 4 - Train set - Per instance ELBO: 101.5824\n",
            "====> Epoch: 4 - Test set - Per Instance ELBO: 100.5219\n",
            "Train Epoch: 5 (5%)\tELBO: 97.781662\n",
            "Train Epoch: 5 (11%)\tELBO: 97.292084\n",
            "Train Epoch: 5 (16%)\tELBO: 95.421219\n",
            "Train Epoch: 5 (21%)\tELBO: 91.570160\n",
            "Train Epoch: 5 (27%)\tELBO: 102.639999\n",
            "Train Epoch: 5 (32%)\tELBO: 95.217751\n",
            "Train Epoch: 5 (37%)\tELBO: 103.606079\n",
            "Train Epoch: 5 (43%)\tELBO: 97.046478\n",
            "Train Epoch: 5 (48%)\tELBO: 93.314079\n",
            "Train Epoch: 5 (53%)\tELBO: 100.981049\n",
            "Train Epoch: 5 (59%)\tELBO: 105.227753\n",
            "Train Epoch: 5 (64%)\tELBO: 101.694168\n",
            "Train Epoch: 5 (69%)\tELBO: 103.085930\n",
            "Train Epoch: 5 (75%)\tELBO: 90.413773\n",
            "Train Epoch: 5 (80%)\tELBO: 96.085037\n",
            "Train Epoch: 5 (85%)\tELBO: 98.928589\n",
            "Train Epoch: 5 (91%)\tELBO: 101.947006\n",
            "Train Epoch: 5 (96%)\tELBO: 92.966980\n",
            "====> Epoch: 5 - Train set - Per instance ELBO: 99.6081\n",
            "====> Epoch: 5 - Test set - Per Instance ELBO: 98.6437\n",
            "Train Epoch: 6 (1%)\tELBO: 99.384277\n",
            "Train Epoch: 6 (7%)\tELBO: 92.142792\n",
            "Train Epoch: 6 (12%)\tELBO: 104.752716\n",
            "Train Epoch: 6 (17%)\tELBO: 98.056732\n",
            "Train Epoch: 6 (23%)\tELBO: 96.219780\n",
            "Train Epoch: 6 (28%)\tELBO: 107.872849\n",
            "Train Epoch: 6 (33%)\tELBO: 100.243835\n",
            "Train Epoch: 6 (39%)\tELBO: 90.546074\n",
            "Train Epoch: 6 (44%)\tELBO: 103.163406\n",
            "Train Epoch: 6 (49%)\tELBO: 93.207359\n",
            "Train Epoch: 6 (55%)\tELBO: 106.298050\n",
            "Train Epoch: 6 (60%)\tELBO: 109.050964\n",
            "Train Epoch: 6 (65%)\tELBO: 96.661819\n",
            "Train Epoch: 6 (71%)\tELBO: 100.804352\n",
            "Train Epoch: 6 (76%)\tELBO: 90.890045\n",
            "Train Epoch: 6 (81%)\tELBO: 90.287338\n",
            "Train Epoch: 6 (87%)\tELBO: 102.285095\n",
            "Train Epoch: 6 (92%)\tELBO: 96.694382\n",
            "Train Epoch: 6 (97%)\tELBO: 98.140671\n",
            "====> Epoch: 6 - Train set - Per instance ELBO: 98.1931\n",
            "====> Epoch: 6 - Test set - Per Instance ELBO: 97.5742\n",
            "Train Epoch: 7 (3%)\tELBO: 101.241150\n",
            "Train Epoch: 7 (8%)\tELBO: 106.242096\n",
            "Train Epoch: 7 (13%)\tELBO: 96.205513\n",
            "Train Epoch: 7 (19%)\tELBO: 103.091034\n",
            "Train Epoch: 7 (24%)\tELBO: 91.539246\n",
            "Train Epoch: 7 (29%)\tELBO: 95.091560\n",
            "Train Epoch: 7 (35%)\tELBO: 94.767471\n",
            "Train Epoch: 7 (40%)\tELBO: 86.235977\n",
            "Train Epoch: 7 (45%)\tELBO: 101.691551\n",
            "Train Epoch: 7 (51%)\tELBO: 99.859215\n",
            "Train Epoch: 7 (56%)\tELBO: 102.598907\n",
            "Train Epoch: 7 (61%)\tELBO: 88.850731\n",
            "Train Epoch: 7 (67%)\tELBO: 100.357994\n",
            "Train Epoch: 7 (72%)\tELBO: 101.643974\n",
            "Train Epoch: 7 (77%)\tELBO: 96.332825\n",
            "Train Epoch: 7 (83%)\tELBO: 100.423904\n",
            "Train Epoch: 7 (88%)\tELBO: 93.427856\n",
            "Train Epoch: 7 (93%)\tELBO: 92.418633\n",
            "Train Epoch: 7 (99%)\tELBO: 92.812943\n",
            "====> Epoch: 7 - Train set - Per instance ELBO: 97.1084\n",
            "====> Epoch: 7 - Test set - Per Instance ELBO: 96.2565\n",
            "Train Epoch: 8 (4%)\tELBO: 101.343948\n",
            "Train Epoch: 8 (9%)\tELBO: 96.315781\n",
            "Train Epoch: 8 (15%)\tELBO: 98.751900\n",
            "Train Epoch: 8 (20%)\tELBO: 85.693329\n",
            "Train Epoch: 8 (25%)\tELBO: 100.884445\n",
            "Train Epoch: 8 (31%)\tELBO: 97.742241\n",
            "Train Epoch: 8 (36%)\tELBO: 99.917946\n",
            "Train Epoch: 8 (41%)\tELBO: 103.723442\n",
            "Train Epoch: 8 (47%)\tELBO: 90.365440\n",
            "Train Epoch: 8 (52%)\tELBO: 90.505104\n",
            "Train Epoch: 8 (57%)\tELBO: 97.095871\n",
            "Train Epoch: 8 (63%)\tELBO: 93.516525\n",
            "Train Epoch: 8 (68%)\tELBO: 89.840096\n",
            "Train Epoch: 8 (73%)\tELBO: 98.270287\n",
            "Train Epoch: 8 (79%)\tELBO: 101.360397\n",
            "Train Epoch: 8 (84%)\tELBO: 98.558914\n",
            "Train Epoch: 8 (89%)\tELBO: 98.026123\n",
            "Train Epoch: 8 (95%)\tELBO: 99.623398\n",
            "Train Epoch: 8 (100%)\tELBO: 93.728088\n",
            "====> Epoch: 8 - Train set - Per instance ELBO: 96.3865\n",
            "====> Epoch: 8 - Test set - Per Instance ELBO: 95.7203\n",
            "Train Epoch: 9 (5%)\tELBO: 96.638512\n",
            "Train Epoch: 9 (11%)\tELBO: 94.358803\n",
            "Train Epoch: 9 (16%)\tELBO: 94.752655\n",
            "Train Epoch: 9 (21%)\tELBO: 92.371147\n",
            "Train Epoch: 9 (27%)\tELBO: 88.956688\n",
            "Train Epoch: 9 (32%)\tELBO: 94.514709\n",
            "Train Epoch: 9 (37%)\tELBO: 90.206879\n",
            "Train Epoch: 9 (43%)\tELBO: 87.829178\n",
            "Train Epoch: 9 (48%)\tELBO: 91.942505\n",
            "Train Epoch: 9 (53%)\tELBO: 92.741470\n",
            "Train Epoch: 9 (59%)\tELBO: 96.472374\n",
            "Train Epoch: 9 (64%)\tELBO: 85.238022\n",
            "Train Epoch: 9 (69%)\tELBO: 95.093040\n",
            "Train Epoch: 9 (75%)\tELBO: 89.924957\n",
            "Train Epoch: 9 (80%)\tELBO: 103.046448\n",
            "Train Epoch: 9 (85%)\tELBO: 97.042572\n",
            "Train Epoch: 9 (91%)\tELBO: 98.337074\n",
            "Train Epoch: 9 (96%)\tELBO: 99.473801\n",
            "====> Epoch: 9 - Train set - Per instance ELBO: 95.6964\n",
            "====> Epoch: 9 - Test set - Per Instance ELBO: 95.1133\n",
            "Train Epoch: 10 (1%)\tELBO: 93.059158\n",
            "Train Epoch: 10 (7%)\tELBO: 99.469177\n",
            "Train Epoch: 10 (12%)\tELBO: 94.067352\n",
            "Train Epoch: 10 (17%)\tELBO: 96.624039\n",
            "Train Epoch: 10 (23%)\tELBO: 95.011734\n",
            "Train Epoch: 10 (28%)\tELBO: 100.714783\n",
            "Train Epoch: 10 (33%)\tELBO: 104.868118\n",
            "Train Epoch: 10 (39%)\tELBO: 99.139862\n",
            "Train Epoch: 10 (44%)\tELBO: 97.135872\n",
            "Train Epoch: 10 (49%)\tELBO: 92.215309\n",
            "Train Epoch: 10 (55%)\tELBO: 92.896194\n",
            "Train Epoch: 10 (60%)\tELBO: 84.943031\n",
            "Train Epoch: 10 (65%)\tELBO: 99.518890\n",
            "Train Epoch: 10 (71%)\tELBO: 95.178162\n",
            "Train Epoch: 10 (76%)\tELBO: 90.786758\n",
            "Train Epoch: 10 (81%)\tELBO: 104.549049\n",
            "Train Epoch: 10 (87%)\tELBO: 91.094933\n",
            "Train Epoch: 10 (92%)\tELBO: 93.672180\n",
            "Train Epoch: 10 (97%)\tELBO: 91.434311\n",
            "====> Epoch: 10 - Train set - Per instance ELBO: 95.1966\n",
            "====> Epoch: 10 - Test set - Per Instance ELBO: 94.8087\n",
            "Train Epoch: 11 (3%)\tELBO: 91.881523\n",
            "Train Epoch: 11 (8%)\tELBO: 99.698677\n",
            "Train Epoch: 11 (13%)\tELBO: 89.679810\n",
            "Train Epoch: 11 (19%)\tELBO: 94.378372\n",
            "Train Epoch: 11 (24%)\tELBO: 91.020370\n",
            "Train Epoch: 11 (29%)\tELBO: 95.222900\n",
            "Train Epoch: 11 (35%)\tELBO: 89.346054\n",
            "Train Epoch: 11 (40%)\tELBO: 85.380959\n",
            "Train Epoch: 11 (45%)\tELBO: 103.769150\n",
            "Train Epoch: 11 (51%)\tELBO: 94.877197\n",
            "Train Epoch: 11 (56%)\tELBO: 98.450226\n",
            "Train Epoch: 11 (61%)\tELBO: 95.257957\n",
            "Train Epoch: 11 (67%)\tELBO: 89.600647\n",
            "Train Epoch: 11 (72%)\tELBO: 89.953430\n",
            "Train Epoch: 11 (77%)\tELBO: 104.656387\n",
            "Train Epoch: 11 (83%)\tELBO: 95.669502\n",
            "Train Epoch: 11 (88%)\tELBO: 89.749748\n",
            "Train Epoch: 11 (93%)\tELBO: 92.886230\n",
            "Train Epoch: 11 (99%)\tELBO: 88.204651\n",
            "====> Epoch: 11 - Train set - Per instance ELBO: 94.7314\n",
            "====> Epoch: 11 - Test set - Per Instance ELBO: 94.8039\n",
            "Train Epoch: 12 (4%)\tELBO: 97.348816\n",
            "Train Epoch: 12 (9%)\tELBO: 88.456978\n",
            "Train Epoch: 12 (15%)\tELBO: 95.530136\n",
            "Train Epoch: 12 (20%)\tELBO: 100.882172\n",
            "Train Epoch: 12 (25%)\tELBO: 94.874519\n",
            "Train Epoch: 12 (31%)\tELBO: 93.878616\n",
            "Train Epoch: 12 (36%)\tELBO: 89.270363\n",
            "Train Epoch: 12 (41%)\tELBO: 92.662613\n",
            "Train Epoch: 12 (47%)\tELBO: 93.306778\n",
            "Train Epoch: 12 (52%)\tELBO: 98.216934\n",
            "Train Epoch: 12 (57%)\tELBO: 93.540253\n",
            "Train Epoch: 12 (63%)\tELBO: 91.784637\n",
            "Train Epoch: 12 (68%)\tELBO: 93.165947\n",
            "Train Epoch: 12 (73%)\tELBO: 96.222404\n",
            "Train Epoch: 12 (79%)\tELBO: 97.141708\n",
            "Train Epoch: 12 (84%)\tELBO: 100.537628\n",
            "Train Epoch: 12 (89%)\tELBO: 99.334366\n",
            "Train Epoch: 12 (95%)\tELBO: 98.682022\n",
            "Train Epoch: 12 (100%)\tELBO: 95.143753\n",
            "====> Epoch: 12 - Train set - Per instance ELBO: 94.4021\n",
            "====> Epoch: 12 - Test set - Per Instance ELBO: 93.8372\n",
            "Train Epoch: 13 (5%)\tELBO: 91.894409\n",
            "Train Epoch: 13 (11%)\tELBO: 100.783371\n",
            "Train Epoch: 13 (16%)\tELBO: 91.339470\n",
            "Train Epoch: 13 (21%)\tELBO: 94.053192\n",
            "Train Epoch: 13 (27%)\tELBO: 90.379646\n",
            "Train Epoch: 13 (32%)\tELBO: 90.009117\n",
            "Train Epoch: 13 (37%)\tELBO: 99.074104\n",
            "Train Epoch: 13 (43%)\tELBO: 93.529732\n",
            "Train Epoch: 13 (48%)\tELBO: 90.419586\n",
            "Train Epoch: 13 (53%)\tELBO: 98.363434\n",
            "Train Epoch: 13 (59%)\tELBO: 96.828247\n",
            "Train Epoch: 13 (64%)\tELBO: 93.901039\n",
            "Train Epoch: 13 (69%)\tELBO: 89.497452\n",
            "Train Epoch: 13 (75%)\tELBO: 92.151901\n",
            "Train Epoch: 13 (80%)\tELBO: 91.570023\n",
            "Train Epoch: 13 (85%)\tELBO: 103.632637\n",
            "Train Epoch: 13 (91%)\tELBO: 103.325653\n",
            "Train Epoch: 13 (96%)\tELBO: 92.902512\n",
            "====> Epoch: 13 - Train set - Per instance ELBO: 94.0593\n",
            "====> Epoch: 13 - Test set - Per Instance ELBO: 93.8726\n",
            "Train Epoch: 14 (1%)\tELBO: 94.415527\n",
            "Train Epoch: 14 (7%)\tELBO: 95.457428\n",
            "Train Epoch: 14 (12%)\tELBO: 88.640335\n",
            "Train Epoch: 14 (17%)\tELBO: 92.728989\n",
            "Train Epoch: 14 (23%)\tELBO: 90.705284\n",
            "Train Epoch: 14 (28%)\tELBO: 91.168839\n",
            "Train Epoch: 14 (33%)\tELBO: 93.845535\n",
            "Train Epoch: 14 (39%)\tELBO: 87.334541\n",
            "Train Epoch: 14 (44%)\tELBO: 95.616188\n",
            "Train Epoch: 14 (49%)\tELBO: 92.825439\n",
            "Train Epoch: 14 (55%)\tELBO: 96.798813\n",
            "Train Epoch: 14 (60%)\tELBO: 92.515205\n",
            "Train Epoch: 14 (65%)\tELBO: 98.413094\n",
            "Train Epoch: 14 (71%)\tELBO: 100.090134\n",
            "Train Epoch: 14 (76%)\tELBO: 98.668472\n",
            "Train Epoch: 14 (81%)\tELBO: 93.224831\n",
            "Train Epoch: 14 (87%)\tELBO: 100.123947\n",
            "Train Epoch: 14 (92%)\tELBO: 99.664192\n",
            "Train Epoch: 14 (97%)\tELBO: 95.186996\n",
            "====> Epoch: 14 - Train set - Per instance ELBO: 93.7052\n",
            "====> Epoch: 14 - Test set - Per Instance ELBO: 93.2449\n",
            "Train Epoch: 15 (3%)\tELBO: 88.152634\n",
            "Train Epoch: 15 (8%)\tELBO: 88.246124\n",
            "Train Epoch: 15 (13%)\tELBO: 92.937584\n",
            "Train Epoch: 15 (19%)\tELBO: 84.892738\n",
            "Train Epoch: 15 (24%)\tELBO: 92.227905\n",
            "Train Epoch: 15 (29%)\tELBO: 100.327187\n",
            "Train Epoch: 15 (35%)\tELBO: 92.689377\n",
            "Train Epoch: 15 (40%)\tELBO: 94.552902\n",
            "Train Epoch: 15 (45%)\tELBO: 86.757103\n",
            "Train Epoch: 15 (51%)\tELBO: 100.813400\n",
            "Train Epoch: 15 (56%)\tELBO: 98.019867\n",
            "Train Epoch: 15 (61%)\tELBO: 99.651367\n",
            "Train Epoch: 15 (67%)\tELBO: 94.701736\n",
            "Train Epoch: 15 (72%)\tELBO: 91.061081\n",
            "Train Epoch: 15 (77%)\tELBO: 90.575912\n",
            "Train Epoch: 15 (83%)\tELBO: 93.474182\n",
            "Train Epoch: 15 (88%)\tELBO: 92.449814\n",
            "Train Epoch: 15 (93%)\tELBO: 93.660866\n",
            "Train Epoch: 15 (99%)\tELBO: 91.474663\n",
            "====> Epoch: 15 - Train set - Per instance ELBO: 93.4006\n",
            "====> Epoch: 15 - Test set - Per Instance ELBO: 93.3327\n",
            "Train Epoch: 16 (4%)\tELBO: 94.771378\n",
            "Train Epoch: 16 (9%)\tELBO: 96.657318\n",
            "Train Epoch: 16 (15%)\tELBO: 89.500458\n",
            "Train Epoch: 16 (20%)\tELBO: 97.197594\n",
            "Train Epoch: 16 (25%)\tELBO: 98.846786\n",
            "Train Epoch: 16 (31%)\tELBO: 95.787491\n",
            "Train Epoch: 16 (36%)\tELBO: 88.165932\n",
            "Train Epoch: 16 (41%)\tELBO: 97.196564\n",
            "Train Epoch: 16 (47%)\tELBO: 84.949478\n",
            "Train Epoch: 16 (52%)\tELBO: 97.742271\n",
            "Train Epoch: 16 (57%)\tELBO: 88.185287\n",
            "Train Epoch: 16 (63%)\tELBO: 98.236305\n",
            "Train Epoch: 16 (68%)\tELBO: 93.176056\n",
            "Train Epoch: 16 (73%)\tELBO: 89.705200\n",
            "Train Epoch: 16 (79%)\tELBO: 88.190781\n",
            "Train Epoch: 16 (84%)\tELBO: 100.551407\n",
            "Train Epoch: 16 (89%)\tELBO: 94.003235\n",
            "Train Epoch: 16 (95%)\tELBO: 97.666229\n",
            "Train Epoch: 16 (100%)\tELBO: 101.573639\n",
            "====> Epoch: 16 - Train set - Per instance ELBO: 93.1918\n",
            "====> Epoch: 16 - Test set - Per Instance ELBO: 93.1533\n",
            "Train Epoch: 17 (5%)\tELBO: 94.623497\n",
            "Train Epoch: 17 (11%)\tELBO: 87.870468\n",
            "Train Epoch: 17 (16%)\tELBO: 97.259018\n",
            "Train Epoch: 17 (21%)\tELBO: 90.770508\n",
            "Train Epoch: 17 (27%)\tELBO: 90.036934\n",
            "Train Epoch: 17 (32%)\tELBO: 88.529793\n",
            "Train Epoch: 17 (37%)\tELBO: 88.162186\n",
            "Train Epoch: 17 (43%)\tELBO: 91.927750\n",
            "Train Epoch: 17 (48%)\tELBO: 85.433060\n",
            "Train Epoch: 17 (53%)\tELBO: 101.497726\n",
            "Train Epoch: 17 (59%)\tELBO: 97.593811\n",
            "Train Epoch: 17 (64%)\tELBO: 93.261047\n",
            "Train Epoch: 17 (69%)\tELBO: 99.434792\n",
            "Train Epoch: 17 (75%)\tELBO: 91.528160\n",
            "Train Epoch: 17 (80%)\tELBO: 103.103256\n",
            "Train Epoch: 17 (85%)\tELBO: 95.858490\n",
            "Train Epoch: 17 (91%)\tELBO: 94.876610\n",
            "Train Epoch: 17 (96%)\tELBO: 96.741783\n",
            "====> Epoch: 17 - Train set - Per instance ELBO: 92.9830\n",
            "====> Epoch: 17 - Test set - Per Instance ELBO: 92.6024\n",
            "Train Epoch: 18 (1%)\tELBO: 92.641190\n",
            "Train Epoch: 18 (7%)\tELBO: 87.724487\n",
            "Train Epoch: 18 (12%)\tELBO: 91.933655\n",
            "Train Epoch: 18 (17%)\tELBO: 89.218529\n",
            "Train Epoch: 18 (23%)\tELBO: 92.653961\n",
            "Train Epoch: 18 (28%)\tELBO: 95.845329\n",
            "Train Epoch: 18 (33%)\tELBO: 96.119858\n",
            "Train Epoch: 18 (39%)\tELBO: 93.332977\n",
            "Train Epoch: 18 (44%)\tELBO: 91.621902\n",
            "Train Epoch: 18 (49%)\tELBO: 92.035599\n",
            "Train Epoch: 18 (55%)\tELBO: 99.656151\n",
            "Train Epoch: 18 (60%)\tELBO: 91.766151\n",
            "Train Epoch: 18 (65%)\tELBO: 88.985558\n",
            "Train Epoch: 18 (71%)\tELBO: 88.589165\n",
            "Train Epoch: 18 (76%)\tELBO: 94.701408\n",
            "Train Epoch: 18 (81%)\tELBO: 86.803856\n",
            "Train Epoch: 18 (87%)\tELBO: 87.215446\n",
            "Train Epoch: 18 (92%)\tELBO: 99.610268\n",
            "Train Epoch: 18 (97%)\tELBO: 91.092476\n",
            "====> Epoch: 18 - Train set - Per instance ELBO: 92.7334\n",
            "====> Epoch: 18 - Test set - Per Instance ELBO: 92.4035\n",
            "Train Epoch: 19 (3%)\tELBO: 91.687965\n",
            "Train Epoch: 19 (8%)\tELBO: 100.729355\n",
            "Train Epoch: 19 (13%)\tELBO: 92.855026\n",
            "Train Epoch: 19 (19%)\tELBO: 96.099487\n",
            "Train Epoch: 19 (24%)\tELBO: 95.422455\n",
            "Train Epoch: 19 (29%)\tELBO: 86.892044\n",
            "Train Epoch: 19 (35%)\tELBO: 89.326965\n",
            "Train Epoch: 19 (40%)\tELBO: 87.122925\n",
            "Train Epoch: 19 (45%)\tELBO: 91.262482\n",
            "Train Epoch: 19 (51%)\tELBO: 89.054070\n",
            "Train Epoch: 19 (56%)\tELBO: 91.717850\n",
            "Train Epoch: 19 (61%)\tELBO: 94.408661\n",
            "Train Epoch: 19 (67%)\tELBO: 89.966522\n",
            "Train Epoch: 19 (72%)\tELBO: 91.048843\n",
            "Train Epoch: 19 (77%)\tELBO: 88.762138\n",
            "Train Epoch: 19 (83%)\tELBO: 97.658295\n",
            "Train Epoch: 19 (88%)\tELBO: 93.731728\n",
            "Train Epoch: 19 (93%)\tELBO: 90.493263\n",
            "Train Epoch: 19 (99%)\tELBO: 87.979385\n",
            "====> Epoch: 19 - Train set - Per instance ELBO: 92.5046\n",
            "====> Epoch: 19 - Test set - Per Instance ELBO: 91.8349\n",
            "Train Epoch: 20 (4%)\tELBO: 93.210236\n",
            "Train Epoch: 20 (9%)\tELBO: 86.643623\n",
            "Train Epoch: 20 (15%)\tELBO: 92.896370\n",
            "Train Epoch: 20 (20%)\tELBO: 91.337814\n",
            "Train Epoch: 20 (25%)\tELBO: 92.616501\n",
            "Train Epoch: 20 (31%)\tELBO: 85.885864\n",
            "Train Epoch: 20 (36%)\tELBO: 96.324699\n",
            "Train Epoch: 20 (41%)\tELBO: 96.142990\n",
            "Train Epoch: 20 (47%)\tELBO: 91.924957\n",
            "Train Epoch: 20 (52%)\tELBO: 89.304871\n",
            "Train Epoch: 20 (57%)\tELBO: 96.729073\n",
            "Train Epoch: 20 (63%)\tELBO: 83.726501\n",
            "Train Epoch: 20 (68%)\tELBO: 90.167648\n",
            "Train Epoch: 20 (73%)\tELBO: 87.224747\n",
            "Train Epoch: 20 (79%)\tELBO: 89.462570\n",
            "Train Epoch: 20 (84%)\tELBO: 90.639664\n",
            "Train Epoch: 20 (89%)\tELBO: 96.435349\n",
            "Train Epoch: 20 (95%)\tELBO: 96.187195\n",
            "Train Epoch: 20 (100%)\tELBO: 97.858116\n",
            "====> Epoch: 20 - Train set - Per instance ELBO: 92.3515\n",
            "====> Epoch: 20 - Test set - Per Instance ELBO: 91.8392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L2PZ4NcVgmIx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simulated output"
      ]
    },
    {
      "metadata": {
        "id": "CeEPFbFcZY9-",
        "colab_type": "code",
        "outputId": "fc2ead34-b7c2-4250-f2ff-c8b31527a222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "for batch_idx, (data, _) in enumerate(test_loader):\n",
        "    break\n",
        "    \n",
        "data.to(device)\n",
        "mu, log_var = model.encode(data)\n",
        "std = 1e-7 + torch.exp(log_var / 2)\n",
        "tmp_list = [data]\n",
        "for _ in range(10):\n",
        "    e = torch.randn((32, 100))\n",
        "    z = mu + std * e.to(device)\n",
        "    # DECODE\n",
        "    tmp_list.append(model.decode(z).cpu())\n",
        "    \n",
        "out = torch.cat(tmp_list)\n",
        "\n",
        "save_image(out.cpu(), r'/content/simulated_output.png',  nrow=32)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0roNHh-xUfLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Log-likelihood estimation"
      ]
    },
    {
      "metadata": {
        "id": "tJdQFjVlUfgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_likelihood(loader, model, k):\n",
        "    prior = Normal(torch.zeros([100], device=device), \n",
        "                   torch.ones([100], device=device))\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        cum_log_p = 0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(loader):\n",
        "\n",
        "            if data.size(0) == 32:\n",
        "\n",
        "                _, x_hat, mu, std = model(data)\n",
        "\n",
        "                data.to(device)\n",
        "                x_hat.to(device)\n",
        "                mu.to(device)\n",
        "                std.to(device)\n",
        "\n",
        "                ExpSumLog = torch.FloatTensor(k, 32).to(device)\n",
        "\n",
        "                for i in range(k):\n",
        "\n",
        "                    e = torch.randn(32, 100).to(model.device)\n",
        "                    z = mu + std * e # z \\sim q(z|x)\n",
        "                    x_hat = model.decode(z).to(device)\n",
        "\n",
        "                    # prob of z according to the prior p(z)\n",
        "                    log_p_z = prior.log_prob(z).sum(-1)  \n",
        "                    # prob of z according to q(z|x)  \n",
        "                    log_q_z_x = Normal(mu, std).log_prob(z).sum(-1)       \n",
        "                    # prob of x|z\n",
        "                    log_p_x_z = nn.functional.binary_cross_entropy(\n",
        "                        x_hat.to(device),\n",
        "                        data.to(device),\n",
        "                        reduction='none'\n",
        "                    ).sum([-1, -2, -3]) # prob of x|z\n",
        "\n",
        "                    ExpSumLog[i, :] = -log_p_x_z + log_p_z - log_q_z_x\n",
        "\n",
        "                MaxESL, _ = ExpSumLog.max(0)\n",
        "\n",
        "                # vector log(p(x_1)), log(p(x_2)), ..., log(p(x_M))\n",
        "                log_p = torch.log(torch.exp(ExpSumLog - MaxESL).sum(0)) \\\n",
        "                + MaxESL\\\n",
        "                - torch.log(torch.FloatTensor([k])).to(device)    \n",
        "\n",
        "                cum_log_p += log_p.sum()     \n",
        "\n",
        "        print('====> Log-likelihood: {:.4f}'.format(\n",
        "            cum_log_p / len(loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XTWaQdlhEPuZ",
        "colab_type": "code",
        "outputId": "a0c3fbbb-2bf1-4613-f6ef-cdda55e33970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "k = 200\n",
        "print(\"Training set log-likelihood:\")\n",
        "log_likelihood(train_loader, model, k)\n",
        "print(\"Test set log-likelihood\")\n",
        "log_likelihood(test_loader, model, k)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set log-likelihood:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Log-likelihood: -86.8304\n",
            "Test set log-likelihood\n",
            "====> Log-likelihood: -86.7261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RiTSOYraiSjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}